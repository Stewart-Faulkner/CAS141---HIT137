from transformers import AutoTokenizer
from collections import Counter
import csv

# File paths
input_file = r'K:\New folder\Assignment 2\output.txt'
output_csv_3_2 = r'K:\New folder\Assignment 2\top_30_tokens.csv'

# 3.2: Use AutoTokenizer to calculate unique tokens
tokenizer = AutoTokenizer.from_pretrained("monologg/biobert_v1.1_pubmed")
token_counter = Counter()

def is_valid_token(token):
    # Check if the token is a valid word or character
    return not token.isdigit() and token.isalpha() and token.strip() != ''

# Read and process the input file
with open(input_file, 'r', encoding='utf-8') as f:
    for line in f:
        # Tokenize each line
        tokens = tokenizer.tokenize(line)
        # Filter out valid tokens
        valid_tokens = [token for token in tokens if is_valid_token(token)]
        # Update token count
        token_counter.update(valid_tokens)

# Get the top 30 most frequent tokens
top_30_tokens = token_counter.most_common(30)

# Write the top 30 tokens to a CSV file
with open(output_csv_3_2, 'w', newline='', encoding='utf-8') as csvfile:
    writer = csv.writer(csvfile)
    writer.writerow(['Token', 'Count'])
    writer.writerows(top_30_tokens)

print(f"Top 30 tokens have been written to {output_csv_3_2}")